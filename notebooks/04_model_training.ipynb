{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers accelerate datasets evaluate numpy pandas jupyter scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/processed/cleaned_classified_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[df.columns[0], df.columns[1], df.columns[9]]]\n",
    "df.columns = ['url', 'title', 'category']\n",
    "dataset = Dataset.from_pandas(df)\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=1) \n",
    "dataset_dict = DatasetDict({\n",
    "    'train': split_dataset['train'],\n",
    "    'validation': split_dataset['test']\n",
    "})\n",
    "print(f\"Train size: {len(dataset_dict['train'])}, Validation size: {len(dataset_dict['validation'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_path = \"Alibaba-NLP/gte-modernbert-base\" # this is where we load our base model in this case we are using a finetuned bert model for embedding tasks\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Define the labels\n",
    "labels = [\n",
    "    \"News\", \"Entertainment\", \"Shop\", \"Chat\", \"Education\",\n",
    "    \"Government\", \"Health\", \"Technology\", \"Work\", \"Travel\", \"Uncategorized\"\n",
    "]\n",
    "num_labels = len(labels)\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all base model parameters initially\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_layers = 22 # ModernBERT has 22 layers\n",
    "\n",
    "# Unfreeze the last n encoder layers\n",
    "for i in range(num_layers - 4, num_layers):  \n",
    "    for param in model.base_model.layers[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Unfreeze the final_norm layer\n",
    "for param in model.base_model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    texts = [f\"{url}:{title}\" for url, title in zip(examples['url'], examples['title'])]\n",
    "    tokenized_inputs = tokenizer(texts, truncation=True, max_length=512)\n",
    "    label_vectors = []\n",
    "    for category in examples['category']:\n",
    "        if isinstance(category, str):\n",
    "            # Remove surrounding brackets (if any) then split on comma\n",
    "            category_clean = category.strip(\"[]\")\n",
    "            cats = [cat.strip() for cat in category_clean.split(',')]\n",
    "        elif isinstance(category, list):\n",
    "            # Already a list; just strip each element\n",
    "            cats = [cat.strip() for cat in category]\n",
    "        else:\n",
    "            cats = []\n",
    "        label_vector = [1.0 if label in cats else 0.0 for label in labels]\n",
    "        label_vectors.append(label_vector)\n",
    "    tokenized_inputs['labels'] = label_vectors\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_data = dataset_dict.map(preprocess_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "roc_auc = evaluate.load(\"roc_auc\", \"multilabel\")\n",
    "#(llm generated this )\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpack evaluation predictions\n",
    "    logits, true_labels = eval_pred\n",
    "    # Convert logits to probabilities using sigmoid\n",
    "    probabilities = 1 / (1 + np.exp(-logits))\n",
    "    # Threshold probabilities at 0.5 to get binary predictions\n",
    "    predictions = (probabilities >= 0.5).astype(np.int32)\n",
    "    true_labels_int = true_labels.astype(np.int32)\n",
    "\n",
    "    # Hamming Loss: fraction of labels incorrectly predicted\n",
    "    hamming = hamming_loss(true_labels_int, predictions)\n",
    "\n",
    "    # Subset Accuracy (Exact Match Ratio)\n",
    "    exact_match = accuracy_score(true_labels_int, predictions)\n",
    "\n",
    "    # Flatten arrays for micro-averaged metrics\n",
    "    predictions_flat = predictions.ravel()\n",
    "    true_labels_flat = true_labels_int.ravel()\n",
    "    probabilities_flat = probabilities.ravel()\n",
    "\n",
    "    # Micro-averaged metrics\n",
    "    precision_micro = precision_score(true_labels_flat, predictions_flat, average='micro')\n",
    "    recall_micro = recall_score(true_labels_flat, predictions_flat, average='micro')\n",
    "    f1_micro = f1_score(true_labels_flat, predictions_flat, average='micro')\n",
    "\n",
    "    # Macro averaged metrics\n",
    "    precision_macro = precision_score(true_labels_int, predictions, average='macro')\n",
    "    recall_macro = recall_score(true_labels_int, predictions, average='macro')\n",
    "    f1_macro = f1_score(true_labels_int, predictions, average='macro')\n",
    "\n",
    "    # Weighted averaged metrics\n",
    "    precision_weighted = precision_score(true_labels_int, predictions, average='weighted')\n",
    "    recall_weighted = recall_score(true_labels_int, predictions, average='weighted')\n",
    "    f1_weighted = f1_score(true_labels_int, predictions, average='weighted')\n",
    "\n",
    "    # ROC-AUC\n",
    "    roc_auc_micro = roc_auc_score(true_labels_flat, probabilities_flat, average='micro')\n",
    "    roc_auc_macro = roc_auc_score(true_labels_int, probabilities, average='macro', multi_class='ovr')\n",
    "\n",
    "    # PR-AUC\n",
    "    pr_auc_micro = average_precision_score(true_labels_flat, probabilities_flat, average='micro')\n",
    "    pr_auc_macro = average_precision_score(true_labels_int, probabilities, average='macro')\n",
    "\n",
    "    # Jaccard Similarity\n",
    "    jaccard_micro = jaccard_score(true_labels_flat, predictions_flat, average='micro')\n",
    "    jaccard_macro = jaccard_score(true_labels_int, predictions, average='macro')\n",
    "\n",
    "    # F1 scores for each label (using the global list 'labels')\n",
    "    f1_per_label = {}\n",
    "    for i, label_name in enumerate(labels):\n",
    "        f1_label = f1_score(true_labels_int[:, i], predictions[:, i], average='binary', zero_division=0)\n",
    "        f1_per_label[f\"f1_{label_name}\"] = round(f1_label, 3)\n",
    "\n",
    "    # Combine all metrics into one dictionary\n",
    "    metrics = {\n",
    "        \"hamming_loss\": round(hamming, 3),\n",
    "        \"exact_match\": round(exact_match, 3),\n",
    "        \"precision_micro\": round(precision_micro, 3),\n",
    "        \"recall_micro\": round(recall_micro, 3),\n",
    "        \"f1_micro\": round(f1_micro, 3),\n",
    "        \"precision_macro\": round(precision_macro, 3),\n",
    "        \"recall_macro\": round(recall_macro, 3),\n",
    "        \"f1_macro\": round(f1_macro, 3),\n",
    "        \"precision_weighted\": round(precision_weighted, 3),\n",
    "        \"recall_weighted\": round(recall_weighted, 3),\n",
    "        \"f1_weighted\": round(f1_weighted, 3),\n",
    "        \"roc_auc_micro\": round(roc_auc_micro, 3),\n",
    "        \"roc_auc_macro\": round(roc_auc_macro, 3),\n",
    "        \"pr_auc_micro\": round(pr_auc_micro, 3),\n",
    "        \"pr_auc_macro\": round(pr_auc_macro, 3),\n",
    "        \"jaccard_micro\": round(jaccard_micro, 3),\n",
    "        \"jaccard_macro\": round(jaccard_macro, 3),\n",
    "    }\n",
    "    # Add per-label F1 scores\n",
    "    metrics.update(f1_per_label)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "lr = 2e-5\n",
    "batch_size = 16 \n",
    "num_epochs = 3 \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"data/models/URL-TITLE-classifier\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')  # Add this if using a compatible GPU\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    tokenizer=tokenizer,  \n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original pre-trained ModernBERT model\n",
    "original_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path, num_labels=num_labels, id2label=id2label, label2id=label2id, problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# Create a Trainer for the original model (no training, just evaluation)\n",
    "original_trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args, \n",
    "    eval_dataset=tokenized_data[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate both models on validation data\n",
    "print(\"Evaluating Your Trained Model...\")\n",
    "your_metrics = trainer.evaluate()\n",
    "print(\"Evaluating Original Pre-trained Model...\")\n",
    "original_metrics = original_trainer.evaluate()\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\nComparison of Models on Validation Data:\")\n",
    "print(f\"{'Metric':<15} {'Your Model':<15} {'Original Model':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for metric in your_metrics:\n",
    "    if metric.startswith(\"eval_\"):\n",
    "        metric_name = metric[5:] \n",
    "        print(f\"{metric_name:<15} {your_metrics[metric]:<15.3f} {original_metrics[metric]:<15.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation dataset that only includes the relevant fields\n",
    "validation_dataset = tokenized_data[\"validation\"]\n",
    "urls = validation_dataset[\"url\"]\n",
    "titles = validation_dataset[\"title\"]\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Lists to store data\n",
    "texts = []\n",
    "urls_list = []\n",
    "titles_list = []\n",
    "true_labels_list = []\n",
    "predicted_labels_list = []\n",
    "prediction_probs_list = []\n",
    "\n",
    "# Process examples individually to avoid padding issues\n",
    "device = model.device if hasattr(model, 'device') else next(model.parameters()).device\n",
    "\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    for i in range(len(validation_dataset)):\n",
    "        # Get a single example\n",
    "        input_ids = torch.tensor(validation_dataset[i][\"input_ids\"]).unsqueeze(0).to(device)  # Add batch dimension\n",
    "        attention_mask = torch.tensor(validation_dataset[i][\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Store url and title\n",
    "        url = urls[i]\n",
    "        title = titles[i]\n",
    "        \n",
    "        # Get true labels\n",
    "        true_label = torch.tensor(validation_dataset[i][\"labels\"]).to(device)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Convert logits to probabilities using sigmoid\n",
    "        prob = torch.sigmoid(logits[0])  # Remove batch dimension\n",
    "        \n",
    "        # Get predicted class (0 or 1) for each label\n",
    "        pred = (prob >= 0.5).int()\n",
    "        \n",
    "        # Store original URL and title\n",
    "        urls_list.append(url)\n",
    "        titles_list.append(title)\n",
    "        \n",
    "        # Decode the input text\n",
    "        text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        texts.append(text)\n",
    "        \n",
    "        true_labels_list.append(true_label.cpu().numpy())\n",
    "        predicted_labels_list.append(pred.cpu().numpy())\n",
    "        prediction_probs_list.append(prob.cpu().numpy())\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{len(validation_dataset)} examples\")\n",
    "\n",
    "# Create DataFrame with results\n",
    "results_df = pd.DataFrame({\n",
    "    'url': urls_list,\n",
    "    'title': titles_list,\n",
    "    'combined_text': texts,\n",
    "})\n",
    "\n",
    "# Add true labels and predictions for each category\n",
    "for i, label_name in enumerate(labels):\n",
    "    results_df[f'true_{label_name}'] = [label_array[i] for label_array in true_labels_list]\n",
    "    results_df[f'pred_{label_name}'] = [pred_array[i] for pred_array in predicted_labels_list]\n",
    "    results_df[f'prob_{label_name}'] = [prob_array[i] for prob_array in prediction_probs_list]\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('validation_results.csv', index=False)\n",
    "print(f\"Results saved to validation_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
